# -*- coding: utf-8 -*-
"""TCC_ACOES_V38.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fboAh7ZUFdWuUbTBS5Qqp8BSrCMPHBVz

#Importação das bibliotecas
"""

# pip install chardet

import chardet
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from numpy import array

import datetime
import csv
import statistics 
import pytz
from math import sqrt
# from multiprocessing import cpu_count
import pickle
from google.colab import drive

# from warnings import catch_warnings
# from warnings import filterwarnings
from sklearn.metrics import mean_squared_error

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
# from sklearn.preprocessing import PowerTransformer
# from sklearn.preprocessing import Normalizer
from sklearn.preprocessing import MaxAbsScaler
from sklearn.preprocessing import RobustScaler
from sklearn.linear_model import LinearRegression

from keras.layers import Bidirectional
from keras.layers import ConvLSTM2D
from keras.layers import Dropout
from keras.layers import Flatten
from keras.layers import LSTM
from keras.layers import RepeatVector
from keras.models import Sequential
from keras.layers import TimeDistributed



# from statsmodels.tsa.arima_model import ARIMA

from keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import ReduceLROnPlateau
# #from tensorflow.keras.callbacks import ModelCheckpoint

from keras.layers.core import Dense
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
# from keras.layers.convolutional import AveragePooling1D

from keras.layers import GRU
from keras.layers import SimpleRNN
from keras.layers import RNN

pip install ta

import ta
from ta import add_all_ta_features


from numpy import hstack
from numpy import concatenate

# import os
# import psutil
# import gc

# from multiprocessing.pool import ThreadPool

import warnings
warnings.filterwarnings("ignore")

def measure_rmse(actual, predicted):
  # print('measure_rmse')
  return sqrt(mean_squared_error(actual, predicted))

def structure_model(cfg,split):
  #buscando o tipo do modelo a ser implementado
  seed = 2016
  np.random.seed(seed)
  tp = cfg[0]
  n_feature,limit,w_pass,o_pass,w_fut,o_fut = split
  model = LinearRegression()
  metric = 'mse'
  # metric = 'accuracy'

  if (tp=='sl1'):
    model = LinearRegression()

  elif (tp=='mlp1'):
    t,b,a,o,l,c,u,k,d,e = cfg
    model = Sequential()
    model.add(Dense(c, activation=a, input_shape=(None,w_pass*n_feature)))
    model.add(Dense(w_fut))
    model.compile(optimizer=o, loss=l, metrics=[metric])
  
  elif (tp=='mlp2'):
    t,b,a,o,l,c,u,k,d,e = cfg
    model = Sequential()
    model.add(Dense(c, activation=a,  input_shape=(None,w_pass*n_feature),use_bias=u,kernel_initializer=k))
    model.add(Dropout(d))
    model.add(Dense(int(c/2),activation=a,kernel_initializer=k))
    model.add(Dense(w_fut))
    model.compile(optimizer=o, loss=l, metrics=[metric])

  elif (tp=='mlp7'):
    t,b,a,o,l,c,u,k,d,e = cfg
    model = Sequential()
    model.add(Dense(c, activation=a,  input_shape=(None,w_pass*n_feature),use_bias=u,kernel_initializer=k))
    model.add(Dropout(d))
    model.add(Dense(int(c/2),activation=a,kernel_initializer=k))
    model.add(Dense(w_fut))
    model.compile(optimizer=o, loss=l, metrics=[metric])

  elif (tp=='mlp3'):
    t,b,a,o,l,c,u,k,d,e = cfg
    model = Sequential()
    model.add(Dense(c, activation=a, input_shape=(None,w_pass*n_feature),use_bias=u,kernel_initializer=k))
    model.add(Dropout(d))
    model.add(Dense(int(c/2),activation=a,kernel_initializer=k))
    model.add(Dropout(d))
    model.add(Dense(w_fut))
    model.compile(optimizer=o, loss=l, metrics=[metric])

  elif (tp=='mlp4'):
    t,b,a,o,l,c,u,k,d,e = cfg
    model = Sequential()
    model.add(Dense(c, activation=a, input_shape=(None,w_pass*n_feature),use_bias=u,kernel_initializer=k))
    model.add(Dropout(d))
    model.add(Dense(c,activation=a,kernel_initializer=k))
    model.add(Dropout(d))
    model.add(Dense(w_fut))
    model.compile(optimizer=o, loss=l, metrics=[metric])

  elif (tp=='mlp5'):
    t,b,a,o,l,c,u,k,d,e = cfg
    model = Sequential()
    model.add(Dense(c, activation=a, input_shape=(None,w_pass*n_feature)))
    model.add(Dense(sqrt(c), activation=a))
    model.add(Dense(w_fut))
    model.compile(optimizer=o, loss=l, metrics=[metric])
  
  elif (tp=='mlp6'):
    t,b,a,o,l,c,u,k,d,e = cfg
    model = Sequential()
    model.add(Dense(c, activation=a, input_shape=(None,w_pass*n_feature)))
    model.add(Dense(sqrt(c), activation=a))
    model.add(Dense(int(sqrt(c)/2), activation=a))
    model.add(Dense(w_fut))
    model.compile(optimizer=o, loss=l, metrics=[metric])

  elif (tp=='cnn1'):
    t,b,f,a,o,c,d,l,e = cfg
    model = Sequential()
    model.add(Conv1D(filters=f, kernel_size=w_pass, activation=a, input_shape=(w_pass,n_feature)))
    model.add(MaxPooling1D(pool_size=1))
    model.add(Flatten())
    model.add(Dense(int(c),activation=a))
    model.add(Dense(w_fut))
    model.compile(optimizer=o, loss=l, metrics=[metric])
    
  elif (tp=='cnn2'):
    t,b,f,a,o,c,d,l,e = cfg
    model = Sequential()
    model.add(Conv1D(filters=f, kernel_size=w_pass, activation=a, input_shape=(w_pass,n_feature)))
    model.add(Conv1D(filters=f, kernel_size=1, activation=a))
    model.add(Dropout(d))
    model.add(MaxPooling1D(pool_size=1))
    model.add(Flatten())
    model.add(Dense(int(c),activation=a))
    model.add(Dense(w_fut))
    model.compile(optimizer=o, loss=l, metrics=[metric])
  
  elif (tp=='lstm1'): #lstm-vanilla
    t,b,f,c,a,o,l,d,e = cfg
    model = Sequential()
    model.add(LSTM(c, activation=a, input_shape=(w_pass,n_feature)))
    model.add(Dense(w_fut))
    model.compile(optimizer=o, loss=l,metrics=[metric])

  elif (tp=='lstm2'): #lstm-stacked
    t,b,f,c,a,o,l,d,e = cfg
    model = Sequential()
    model.add(LSTM(c, activation=a,return_sequences=True, input_shape=(w_pass,n_feature)))
    model.add(LSTM(c, activation=a))
    model.add(Dense(w_fut))
    model.compile(optimizer=o, loss=l, metrics=[metric])

  elif (tp=='lstm3'):  #lstm-stacked + dropout
    t,b,f,c,a,o,l,d,e = cfg
    model = Sequential()
    model.add(LSTM(c, activation=a,return_sequences=True, input_shape=(w_pass,n_feature)))
    model.add(LSTM(c, activation=a,return_sequences=True))
    model.add(Dropout(d))
    model.add(LSTM(int(c/2), activation=a))
    model.add(Dense(w_fut))
    model.compile(optimizer=o, loss=l, metrics=[metric])

  elif (tp=='lstm4'): #lstm-bidirecional
    t,b,f,c,a,o,l,d,e = cfg
    model = Sequential()
    model.add(Bidirectional(LSTM(c, activation=a), input_shape=(w_pass,n_feature)))
    model.add(Dense(w_fut))
    model.compile(optimizer=o, loss=l, metrics=[metric])

  elif (tp=='lstm5'): #lstm-bidirecional + stacked
    t,b,f,c,a,o,l,d,e = cfg
    model = Sequential()
    model.add(Bidirectional(LSTM(c, activation=a,return_sequences=True), input_shape=(w_pass,n_feature)))
    model.add(LSTM(c, activation=a))
    model.add(Dense(w_fut))
    model.compile(optimizer=o, loss=l, metrics=[metric])

  elif (tp=='lstm6'): #cnn-lstm
    t,b,f,c,a,o,l,d,e = cfg
    model = Sequential()
    model.add(TimeDistributed(Conv1D(filters=f, kernel_size=1, activation=a), input_shape=(None,w_pass,n_feature)))
    if (w_pass>2):
      w_pass = 2
    model.add(TimeDistributed(MaxPooling1D(pool_size=w_pass)))
    model.add(TimeDistributed(Flatten()))
    model.add(LSTM(c, activation=a))
    model.add(Dense(w_fut))
    model.compile(optimizer=o, loss=l, metrics=[metric])

  elif (tp=='lstm7'): #ConvLstm
    t,b,f,c,a,o,l,d,e = cfg
    model = Sequential()
    # if (w_pass>1):
    #  dim = int(w_pass/2)
    model.add(ConvLSTM2D(filters=f, kernel_size=(1,w_pass), activation=a, input_shape=(1,1,w_pass,n_feature)))
    model.add(Flatten())
    model.add(Dense(w_fut))
    model.compile(optimizer=o, loss=l, metrics=[metric])

  elif (tp=='lstm8'): #encoder-decoder
    t,b,f,c,a,o,l,d,e = cfg
    model = Sequential()
    model.add(LSTM(c, activation=a, input_shape=(w_pass,n_feature)))
    model.add(RepeatVector(w_fut))
    model.add(LSTM(c, activation=a,return_sequences=True))
    model.add(TimeDistributed(Dense(1)))
    model.compile(optimizer=o, loss=l, metrics=[metric])

  elif (tp=='gru1'):
    t,c,b,a,o,l,d,e = cfg
    model = Sequential()
    model.add(GRU(c,activation=a,input_shape=(w_pass,n_feature)))
    model.add(Dense(w_fut))
    model.compile(optimizer=o, loss=l, metrics=[metric])

  elif (tp=='rnn1'):
    t,c,b,a,o,l,d,e = cfg
    model = Sequential()
    model.add(SimpleRNN(c,activation=a,input_shape=(w_pass,n_feature)))
    model.add(Dense(w_fut))
    model.compile(optimizer=o, loss=l, metrics=[metric])


  else:
    print('___ERRO_structure: Tipo não definido')

  # print('struct_model')
  return model

def model_config(model_):
  parameters = list()


  if (model_ == 'simpleLinear'):
    t_params = ['sl1']
    for t in t_params:
      parameters.append([t])
  
  if (model_ =='mlp'):
    b_params = [x for x in range(10,1000,100)]#batch size
    a_params = ['relu','sigmoid','softmax','softplus','softsign','tanh','selu','elu','exponential'] #Activations 
    o_params = ['SGD','RMSprop','Adam','Adadelta','Adagrad','Adamax','Nadam','Ftrl'] #Optimizers
    # e_params = [x for x in range(10,1000,50)] #Epochs
    l_params = ['mse','mae','mape','msle'] #losses
    c_params = [x**2 for x in range(8,26)] #Units
    k_params = ['zeros','ones','constant','random_normal','random_uniform','truncated_normal','VarianceScaling',
                'orthogonal','identity','lecun_uniform','glorot_normal','glorot_uniform','he_normal','lecun_normal'] #kernel_initializer
    d_params = [x/10 for x in range(1,9)] #dropout
    # t_params =['mlp1','mlp2','mlp3','mlp4','mlp5','mlp6'] #tipos de modelos
    u_params = [False,True] 
  
    # b_params = [250]
    # a_params = ['relu']
    # o_params = ['SGD']
    # l_params = ['mape'] 
    # c_params = [484]
    # u_params = [False]
    # k_params = ['zeros']
    # d_params = [0.7]
    e_params = [100]
    t_params = ['mlp1','mlp2','mlp3','mlp4','mlp5','mlp6']
    parameters = [t_params,b_params,a_params,o_params,l_params,c_params,u_params,k_params,d_params,e_params]
  
  
  if (model_=='cnn'):
    b_params = [x for x in range(10,1000,200)]#batch size
    f_params = [x for x in range(12,504,50)] #Filters
    a_params = ['relu','sigmoid','softmax','softplus','softsign','tanh','selu','elu','exponential'] #Activations 
    o_params = ['SGD','RMSprop','Adam','Adadelta','Adagrad','Adamax','Nadam','Ftrl'] #Optimizers
    c_params = [x**2 for x in range(2,8)] #Units
    d_params = [x/10 for x in range(1,9)] #dropout
    l_params = ['mse','mae','mape','msle'] #losses
    # e_params = [x for x in range(10,1000,50)] #Epochs
    # t_params = ['cnn1','cnn2']       #tipo

    # f_params = [64]           #filters
    # b_params = [250]
    # a_params = ['relu']       #activations
    # o_params = ['adam']       #optimizers
    # c_params = [50]           #Units
    # d_params = [0.7]          #dropout
    # l_params = ['mse']        #losses
    e_params = [200]         #epochs
    t_params = ['cnn1']       #tipos
    parameters = [t_params,b_params,f_params,a_params,o_params,c_params,d_params,l_params,e_params]

  if (model_=='lstm'):
    # b_params = [x for x in range(500,10000,500)]#batch size
    # f_params = [x for x in range(5,504,50)] #Filters
    # a_params = ['relu','sigmoid','softmax','softplus','softsign','tanh','selu','elu','exponential'] #Activations 
    # o_params = ['SGD','RMSprop','Adam','Adadelta','Adagrad','Adamax','Nadam','Ftrl'] #Optimizers
    # c_params = [x**2 for x in range(1,8)] #Units
    # d_params = [x/10 for x in range(1,9)] #dropout
    # l_params = ['mse','mae','mape','msle'] #losses
    # e_params = [x for x in range(10,1000,100)] #Epochs
    # t_params = ['lstm1','lstm2','lstm3','lstm4','lstm5','lstm6','lstm7','lstm8']

    t_params = ['lstm1']
    b_params = [5]
    f_params = [205]
    c_params = [49]
    a_params = ['sigmoid']
    o_params = ['Adam']
    l_params = ['msle']
    d_params = [0.1]
    e_params = [50] 
    parameters=[t_params,b_params,f_params,c_params,a_params,o_params,l_params,d_params,e_params]

  if (model_== 'gru'):
    b_params = [x for x in range(10,1000,100)]#batch size
    a_params = ['relu','sigmoid','softmax','softplus','softsign','tanh','selu','elu','exponential'] #Activations 
    o_params = ['SGD','RMSprop','Adam','Adadelta','Adagrad','Adamax','Nadam','Ftrl'] #Optimizers
    c_params = [x**2 for x in range(8,26)] #Units
    d_params = [x/10 for x in range(1,9)] #dropout
    l_params = ['mse','mae','mape','msle'] #losses
    # e_params = [x for x in range(10,1000,100)] #Epochs
    t_params = ['gru1']

    # c_params = [441]
    # b_params = [250]
    # a_params = ['softplus']
    # o_params = ['Adamax']
    # l_params = ['mse']
    # d_params = [0.7]
    e_params = [200] 
    # t_params = ['gru1']
    parameters=[t_params,b_params,c_params,a_params,o_params,l_params,d_params,e_params]

  if (model_== 'simpleRNN'):
    b_params = [x for x in range(10,1000,100)]#batch size
    a_params = ['relu','sigmoid','softmax','softplus','softsign','tanh','selu','elu','exponential'] #Activations 
    o_params = ['SGD','RMSprop','Adam','Adadelta','Adagrad','Adamax','Nadam','Ftrl'] #Optimizers
    c_params = [x**2 for x in range(8,26)] #Units
    d_params = [x/10 for x in range(1,9)] #dropout
    l_params = ['mse','mae','mape','msle'] #losses
    # e_params = [x for x in range(10,1000,100)] #Epochs
    t_params = ['rnn1']

    # c_params = [441]
    # b_params = [250]
    # a_params = ['softplus']
    # o_params = ['Adamax']
    # l_params = ['mse']
    # d_params = [0.7]
    e_params = [200] 
    # t_params = ['rnn1']
    parameters=[t_params,b_params,c_params,a_params,o_params,l_params,d_params,e_params]


  # print('model_con/fig')
  return parameters

def train_model(x_train,y_train,x_test,y_test,cfg,split,model,v):
  #buscando o modelo a ser realizado a predição
  tp = cfg[0]
  bs = cfg[1]
  e = cfg[-1]

  # monitor = 'mse'
  monitor = 'val_loss'
  # monitor = 'accuracy'

  #callback para reduce plateau
  alpha = 1e-2
  lr_reduce = ReduceLROnPlateau(monitor=monitor, factor=0.01, min_delta=alpha, patience=15, verbose=v)
  #callback para earlyStopping
  ea_stopping=  EarlyStopping(monitor=monitor, mode='auto', patience=20, verbose=v)
  #Agrupamento dos callbacks
  callbacks = [lr_reduce,ea_stopping]

  n_feature,limit,w_pass,o_pass,w_fut,o_fut = split



 #historico
  history=[]
  graphic_h = True

  if (tp=='sl1'):
    # print(x_train.shape, y_train.shape)
    # x_train = x_train.reshape(x_train.shape[0],x_train.shape[2])
    x_train = x_train.reshape(x_train.shape[0],w_pass*n_feature)
    model.fit(x_train,y_train)
    graphic_h = False
  elif (tp=='mlp1' or tp=='mlp2' or tp=='mlp3' or tp=='mlp4' or tp=='mlp5' or tp=='mlp6' ):
    x_train = x_train.reshape(x_train.shape[0],w_pass*n_feature)
    x_test = x_test.reshape(x_test.shape[0],w_pass*n_feature)
    history = model.fit(x_train,y_train, epochs=e, batch_size=bs, validation_data=(x_test, y_test),verbose=v,callbacks=callbacks)
  elif (tp=='cnn1'or tp=='cnn2' or tp=='gru1' or tp=='rnn1'):
    x_train = x_train.reshape(x_train.shape[0],w_pass,n_feature)
    x_test = x_test.reshape(x_test.shape[0],w_pass,n_feature)
    history = model.fit(x_train,y_train, epochs=e, batch_size=bs, validation_data=(x_test, y_test),verbose=v,callbacks=callbacks)
  elif (tp=='lstm1' or tp=='lstm2' or tp=='lstm3' or tp=='lstm4' or tp=='lstm5'):
    x_train = x_train.reshape(x_train.shape[0],w_pass,n_feature)
    x_test = x_test.reshape(x_test.shape[0],w_pass,n_feature)
    history = model.fit(x_train,y_train, epochs=e,batch_size=bs, validation_data=(x_test, y_test), verbose=v,callbacks=callbacks)
  elif (tp=='lstm6'):
    x_train = x_train.reshape(x_train.shape[0],1,w_pass,n_feature)
    x_test = x_test.reshape(x_test.shape[0],1,w_pass,n_feature)
    history = model.fit(x_train,y_train, epochs=e, batch_size=bs, validation_data=(x_test, y_test),verbose=v,callbacks=callbacks)
  elif (tp=='lstm7'):
    x_train = x_train.reshape(x_train.shape[0],1,1,w_pass,n_feature)
    x_test = x_test.reshape(x_test.shape[0],1,1,w_pass,n_feature)
    history = model.fit(x_train,y_train, epochs=e, batch_size=bs, validation_data=(x_test, y_test), verbose=v,callbacks=callbacks)
  elif (tp=='lstm8'):
    x_train = x_train.reshape(x_train.shape[0],w_pass,n_feature)
    y_train = y_train.reshape(y_train.shape[0],w_fut,1)
    x_test = x_test.reshape(x_test.shape[0],w_pass,n_feature)
    y_test = y_test.reshape(y_test.shape[0],w_fut,1)
    history = model.fit(x_train,y_train, epochs=e, batch_size=bs, validation_data=(x_test, y_test), verbose=v,callbacks=callbacks)

  else:
    print('___ERRO_train: Tipo não definido')
  
  # print('train_model')
  return model, history, graphic_h

def predict_model(x_input,cfg,split,model,v):
  #buscando o modelo a ser realzado a predição
  tp = cfg[0]
  result=[]
  n_feature,limit,w_pass,o_pass,w_fut,o_fut = split


  if (tp=='sl1'):
    x_input = x_input.reshape(1,w_pass*n_feature)   
    result=model.predict(x_input)
  elif (tp=='mlp1' or tp=='mlp2' or tp=='mlp3' or tp=='mlp4'or tp=='mlp5' or tp=='mlp6' ):
    x_input = x_input.reshape(1,w_pass*n_feature)
    result=model.predict(x_input,verbose=v)
  elif (tp=='cnn1'or tp=='cnn2' or tp=='lstm1' or tp=='lstm2' or tp=='lstm3' or tp=='lstm4' 
        or tp=='lstm5' or tp=='lstm8' or tp=='gru1' or tp=='rnn1'):
    x_input = x_input.reshape(1,w_pass,n_feature)
    result=model.predict(x_input,verbose=v)
  elif (tp=='lstm6'):
    x_input = x_input.reshape(1,1,w_pass,n_feature)
    result=model.predict(x_input,verbose=v)
  elif (tp=='lstm7'):
    x_input = x_input.reshape(1,1,1,w_pass,n_feature)
    result=model.predict(x_input,verbose=v)

  else:
    print('___ERRO_predict: Tipo não definido')
  
  return result

def model_norms(norm):

  if (norm == 'standarScaler'):
    scaler = StandardScaler()
  elif (norm == 'powerTransformer'):
    scaler = PowerTransformer(copy=True, method='box-cox', standardize=True)
  elif (norm == 'normalizer'):
    scaler = Normalizer()
  elif (norm == 'maxAbsScaler'):
    scaler = MaxAbsScaler()  
  elif (norm == 'minMaxScaler'):
    scaler = MinMaxScaler()
  elif (norm == 'robustScaler'):
    scaler = RobustScaler()
  else:
    print('___ERRO: Tipo de normalizador não definido')

  # print('model_norms')
  return scaler

def salvarResultados(path,resultados,fig,imagemNome,tecnica,ativo,model):
  dadosNome = 'resultadosTeste.csv'
  #salvar resultados
  file =  open(path+dadosNome,'a',newline='')
  write = csv.writer(file,delimiter=',',quoting=csv.QUOTE_MINIMAL)
  write.writerow(resultados)
  file.close()

  #salvar imagem
  fig.savefig(path+'Imagens/'+tecnica+'/'+imagemNome, format='png')

  #salvar modelo
  name = 'model_'+ativo+'_'+tecnica+'.h5'
  # pickle.dump(model, open(path+'/Modelos/'+name, 'wb'))
  model.save(path+'/Modelos/'+name)

def dataset3(value,ativo):
  # 1 - Monta o drive
  drive.mount('/content/gdrive') 

  # 2 - Repassa o endereço do drive
  path = '/content/gdrive/My Drive/TCC/1_Arquivos/' + ativo + '.csv' 

  # 3 - Descobre o tipo de enconding do arquivo
  with open(path, 'rb') as rawdata:
      enc = chardet.detect(rawdata.read(1000))

  # 4 - Carrega os dados e inclui cabeçário)
  df_old = pd.read_csv(path, header=None, delimiter=',', encoding=enc['encoding'],names=["data", "open", "high", "low","close","tick","volume"])
  
  # 5 - Ajusta as colunas
  if (value==1): # Apenas uma feature (valor de fechamento)
    df = df_old[['close']]
  elif (value==2): #Dados completo do dataset
    df = df_old[['open','high','low','tick','volume','close']]
  elif (value==3):# Multifeature
    # Inclui os indicadores analise técnica 
    df = add_all_ta_features(df_old, open="open", high="high", low="low", close="close", volume="volume", fillna=True)
    col = df.columns 
    
    # Coloca a coluna 'close' na ultima posição das colunas (reordena as colunas e reindexa o indice) (target na última posição)
    newCol = []
    for x in range(0,len(col)):
      if (col[x] != 'data'):
        if (col[x] != 'close'):
          newCol.append(col[x])
    newCol.append("close")
    df = df.reindex(columns=newCol)

  print(df.shape)
  # 6 - Verifica quantas features serão trabalhadas
  value = len(df.columns)-1
  df = df.values

  return df, value

def gerarclass(value,taxa):
  if (value < -taxa ):
    #Tendencia de baixa
    return 0
  else:
    #Tendencia e alta
    return 1

def totalSimulacoes(tecnicas,cfg_ciclo,cfg_split,cfg_norm):
  n=0
  for x in tecnicas:
    modelos = model_config(x)
    for y in modelos:
      if (len(y)==1):
        continue
      n += len(y)
  total = ((n*cfg_ciclo[0]+len(cfg_norm))*cfg_ciclo[1]+len(cfg_split))*cfg_ciclo[2]*len(tecnicas)
  print(f'Total de simulações: {total/2}\n\n')
  # print('totalSimulações')

def grade(ativo,cfg_ciclo,tecnicas,cfg_norm,cfg_split,cfg_system):
  global cont
  debug,graphic,salve,verbose,path,dt,cont = cfg_system
  parametroFinal= ''


  #contagem do total de simulações
  # totalSimulacoes(tecnicas,cfg_ciclo,splits,cfg_norm)

  #Técnicas
  print('1 - Ativo: ',ativo)
  
  
  #para cada tipo de features(1-somente um valor, 2-valores do preço e 3-indicadores tecnicos)
  for z in range(2,dt+1):
    print(f' 2 - Tipo de features: {z}')
    #Dataset dados reais
    series, feature = dataset3(z,ativo)

    #Geração dos splits
    splits = model_split(series,feature,cfg_split)
    #split basico
    cfg_split_final = splits[0]
    #normalizador basico
    cfg_norm_final = cfg_norm[0]


    for tecnica in tecnicas:
      print('   3 - Tecnica: ',tecnica)
      cfg_model = model_config(tecnica)
      #totalizador de parametros
      totalParametros = len(cfg_model)
      #Repetição Geral
      for c2 in range(0,cfg_ciclo[2]):
        print('   Ciclo geral: ',c2)
        #modelos de cada técnica
        for modelo in cfg_model[0]:
          print('     4 - Modelo: ',modelo)
          #Recebe os valores default dos parametros, de forma segura, gerando novo objeto
          cfg_model_final = cfg_model[:]
          #Recebe o modelo a ser testado
          cfg_model_final[0] = [modelo]

          #Repetições para retestar os parametros
          for c1 in range(0,cfg_ciclo[1]):
            print('       5 - Ciclo parametros: ',c1)
            
            # Calculado o melhor de cada parametro
            for parametros in range(1,totalParametros):
              #Recebe o valor default do parametro
              cfg_model_final[parametros] = cfg_model[parametros]
              print('         6 - Parametros: ',cfg_model_final[parametros])
            
              #se o parametro de teste tiver somente um valor         
              if (len(cfg_model_final[parametros])==1):
                continue
              #Geração do bloco de teste para o parametro especififco 
              erroBase = 10**10
              for parametro in cfg_model_final[parametros]:
                print(f'          Ativo:{ativo} -Ciclo Geral:{c2} -Modelo:{modelo} -Ciclo Parâmetros:{c1} -Parâmetro Testado:{parametro}  -contador:{cont} >>menor erro:{erroBase} >>melhor parâmetro:{parametroFinal} ')
                #Geração do modelo, predição, erro, gráfico e arquivamento 
                erro,cont = modeloPredicao(ativo,tecnica,series,splits[0],cfg_norm[0],cfg_model_tester(cfg_model_final,parametro,parametros),cfg_ciclo[0],cfg_system,cont)
                #Ordenação dos melhores
                if (erro < erroBase):
                  perc = (1-(erro/erroBase))*100
                  print(f'              Novo parametro: {parametro}, Erro:(de {erroBase:.2f} para {erro:.2f} > {perc:.2f}%)')
                  #atualização do erro
                  erroBase = erro
                  #atualização do melhor parametro
                  parametroFinal = parametro
                
              #Atualizar o valor do parametro
              print(f'Atualizado o parametro {parametros} para {parametroFinal}')
              cfg_model_final[parametros] = [parametroFinal]
              
            #Normalizador
            #se o normalizador tiver somente um valor      
            erroBase = 10**10
            if (len(cfg_norm)==1):
              continue
            for norm in cfg_norm:
              print('         6.1 - Normalizador: ',norm)
              #Geração do modelo, predição, erro, gráfico e arquivamento 
              erro, cont = modeloPredicao(ativo,tecnica,series,splits[0],norm,ajuste_cfg_model_tester(cfg_model_final),cfg_ciclo[0],cfg_system,cont)
              #Ordenação dos melhores
              if (erro < erroBase):
                perc = (1-(erro/erroBase))*100
                print(f'            6.1.1 - Novo normalizador: {norm}, Erro:(de {erroBase:.2f} para {erro:.2f} > {perc:.2f}%)')
                #atualização do erro
                erroBase = erro
                #atualização do melhor parametro
                normalizadorFinal = norm
                
            #Atualizar o valor do parametro
            cfg_norm_final = normalizadorFinal    
          print('     4.1 - Modelo: ',modelo,' - cfg_model_final: ',cfg_model_final)
          #roda no minimo uma vez com os valores padrões ou atualizados
          if (cont == 0):         
            erro,cont = modeloPredicao(ativo,tecnica,series,cfg_split_final,cfg_norm_final,ajuste_cfg_model_tester(cfg_model_final),cfg_ciclo[0],cfg_system,cont)
            cont = 0 

          #Variação os splits
          if (len(splits)==1):
            continue
          for split in splits:
            print('     4.2 - Split: ',split)
            #Geração do modelo, predição, erro, gráfico e arquivament
            erro,cont = modeloPredicao(ativo,tecnica,series,split,cfg_norm_final,cfg_model_final,cfg_ciclo[0],cfg_system,cont)
            #Ordenação dos melhores
            if (erro < erroBase):
              perc = (1-(erro/erroBase))*100
              print(f'      4.2.1 - Novo Split: {split}, Erro:(de {erroBase:.2f} para {erro:.2f} > {perc:.2f}%)')
              #atualização do erro
              erroBase = erro
              #atualização do melhor parametro
              cfg_split_final = split
              
  
  print('Completado a grade!')

def cfg_model_tester(cfg_model,parametro,posicao):
  cfg_model_test=[]
  for x in cfg_model:
    cfg_model_test.append(x[0])
  cfg_model_test[posicao] = parametro
  return cfg_model_test

def ajuste_cfg_model_tester(cfg_model):
  cfg_model_test=[]
  for x in cfg_model:
    cfg_model_test.append(x[0])
  return cfg_model_test

def modeloPredicao(ativo,tecnica,series,split,norm,cfg,repeat,cfg_system,cont):
  #normalizador
  scaler = model_norms(norm)
  #split
  x_train,y_train,x_test,y_test,scale_fit = split_sequence(series,split,scaler)

  #Geração do modelo
  model = structure_model(cfg,split)  

  #Variáveis locais
  error_mean = []
  error = []
  erro = []
  history = []
  graphic_h = False
  predictions_inverse = []

  #repetidor
  for x in range(repeat):
    print('repeat:',x)
    #incrementando o contador universal
    cont +=1
    #treino
    model, history, graphic_h = train_model(x_train,y_train,x_test,y_test,cfg,split,model,cfg_system[3])
    predictions = list()

    n_feature,limit,w_pass,o_pass,w_fut,o_fut = split
    #Verifica para no test os resutados e compila
    for j in range(x_test.shape[0]):
      x_input = x_test[j].reshape(w_pass,n_feature)
      pred = predict_model(x_input,cfg,split,model,0)
      predictions.append(pred)
      # print('j: ',j,'x_test: ',scale_fit.inverse_transform(x_test[j]),'predi: ',scale_fit.inverse_transform(pred),'real: ',y_test[j])
    predictions = array(predictions).reshape(x_test.shape[0],w_fut)
    
    #Calcula o erro nos valores preditos
    if (np.count_nonzero(~np.isnan(predictions))>0): 
      #cria base para inverter os valores normalizados
      prediction_inverse = x_test.reshape((x_test.shape[0],w_pass*n_feature))
      #escoher a quantidade de valores para manter as dimensões de treino
      prediction_inverse = prediction_inverse[:,w_fut-series.shape[-1]:]
      #contanea com os valores preditos para inversão
      prediction_inverse = concatenate((predictions,prediction_inverse), axis=1)
      #inversão dos valores normais            
      predictions_inverse = scale_fit.inverse_transform(prediction_inverse)
      #selecionando somente os valores preditos com valores normais
      predictions_inverse = predictions_inverse[:,0:w_fut]
      erro = []
      #armazenando o erro
      for er in range(y_test.shape[0]):
        err = y_test[er]-predictions_inverse[er]
      erro = array(erro)
      erro = erro.reshape(erro.shape[0]*w_fut)

    #calculando o erro
    error_n = measure_rmse(y_test,predictions_inverse)
    error_mean.append(error_n)

  #Caso não tenha retorno aborta 
  if (len(predictions_inverse)>0):
    #Impressão gráfica entre Real e Predição
    fig = graficos(y_test,predictions_inverse,erro,history,cfg_system,graphic_h)
  else:
    return 10*10, cont
  #Media do erros
  error = round(statistics.mean(error_mean),5)
  #Impressão
  time = datetime.datetime.now(pytz.timezone('America/Fortaleza')).strftime('%d/%m/%Y %H:%M:%S')
  #Salvar resultados
  if (cfg_system[2]):
    figName = ativo + '_imagemTeste_' + cfg[0] + "_" + str(cont)
    resultados = concatenarDados(time,ativo,norm,split,cfg,error,cont)
    salvarResultados(cfg_system[4],resultados,fig,figName,tecnica,ativo,model)

  print(f'          time:{time} >> {ativo} >> {norm} >> {split} >> {cfg} >> erro: {error} >> cont: {cont}')

  return error,cont

def concatenarDados(time,ativo,norm,split,cfg,error,cont):
  resultados = []
  resultados.append(time)
  resultados.append(ativo)
  resultados.append(norm)
  for x in split:
    resultados.append(x)
  for x in cfg:
    resultados.append(x)
  resultados.append(str(error))
  resultados.append(str(cont)) 

  return resultados

def graficos(y_test,predictions_inverse,erro,history,cfg_system,graphic_h):
  gs = plt.GridSpec(2, 3, wspace=0.2, hspace=0.4)
  fig = plt.figure(figsize=(20,8))

  ax1 = fig.add_subplot(gs[0, :])
  ax1.plot(y_test, color = 'black', label = 'Real')
  ax1.plot(predictions_inverse, color = 'red', label = 'Prediction')
  ax1.set_title('Predito X Real')
  ax1.set_xlabel('Time')
  ax1.set_ylabel('Price')
  # plt.grid(True)
  major_ticks = np.arange(0, len(y_test), 10)
  # minor_ticks = np.arange(0, y_test.shape[0]*0.1, 1)
  ax1.set_xticks(major_ticks)
  # ax1.set_xticks(minor_ticks, minor=True)
  ax1.grid(which='both')
  # Or if you want different settings for the grids:
  # ax1.grid(which='minor', alpha=0.2)
  ax1.grid(which='major', alpha=0.5)
  # plt.grid()
  ax1.legend()

  ax2 = fig.add_subplot(gs[1, 0])
  ax2 = sns.distplot(erro)
  ax2.set_title('Histograma - Erros')
  ax2.set_ylabel('Densidade')
  ax2.set_xlabel('erro')

  # ax3 = fig.add_subplot(gs[1, 1]) 
  # ax3.boxplot(erro) 
  # ax3.set_title('BoxPlot - Erros')

  ax4 = fig.add_subplot(gs[1, 1])
  ax4.plot(erro, color = 'black', label = 'Erro')
  ax4.set_title('Erro Residual')
  ax4.set_xlabel('Valor')
  ax4.set_ylabel('Amostras')
  ax4.grid(True)
  ax4.legend()

  if (graphic_h):
    ax5 = fig.add_subplot(gs[1,2])
    ref = range(0,len(history.history['loss']))
    color = 'tab:red'
    ax5.set_title('Erro Treino X Erro Teste')
    ax5.set_xlabel('history')
    ax5.set_ylabel('erro_train', color=color)
    ax5.plot(ref,history.history['loss'], label='train',color=color)
    ax5.tick_params(axis='y', labelcolor=color)

    ax6 = ax5.twinx()  
    color = 'tab:blue'
    ax6.set_ylabel('erro_test', color=color) 
    ax6.plot(ref,history.history['val_loss'], label='test', color=color)
    ax6.tick_params(axis='y', labelcolor=color)
    fig.tight_layout()

  #Mostrar o gráfico
  if (cfg_system[1]):
    plt.show()

  return fig

def resutadosFinais(scores,current_begin):
  print('\n\n_____ Término dos cálculo dos modelos')
  #convertendo em array
  scores = np.array(scores)
  #alterando as dimensões
  scores = scores.reshape(scores.shape[0]*scores.shape[1],scores.shape[2])
  #convertendo em lista
  scores = scores.tolist()
  #colocando em ordem crescente pelo menor erro
  scores.sort()
  #impressão dos melhores modelos
  for score in scores[:10]:
    print(score)

  current_end = datetime.datetime.now()
  print('\n\n______________Término')
  print('Tempo duração Total:',current_end-current_begin)
  # print('resutadosFinais')

def model_split(series,feature,cfg_split):
  splits = list()
  # 1 - Parametros
  sp_mi,sp_ma,wp_mi,wp_ma,wop_mi,wop_ma,wf_mi,wf_ma,wof_mi,wof_ma = cfg_split

  #existe uma complexidade na geração dos splits, 
  #deve ser resolvida antes de ser liberada esta funcionalidade
  wop_mi=0
  wop_ma=0

  # 2 - Define o tamanho do treino e teste
  for s in range(sp_mi,sp_ma+1): #minimo 3,maximo 8
    #2.1 - Limitação da janela de passado, para grandes dataset
    if (wp_ma=='auto'):
      wp_ma = int(len(series)*(s/10))
    #3 - Tamanho da janela do passado
    for wp in range(wp_mi,wp_ma+1):
      #4 - Salto para o passado(offset)
      for wop in (range(wop_mi,wop_ma+1)):
        #5 - Tamanho da janela do futuro
        for wf in range(wf_mi, wf_ma+1):
          #6 - Salto para o futuro(offset)
          for wo in range(wof_mi,wof_ma+1):
            splits.append([feature,s/100,wp,wop,wf,wo])
  #print('model_split')
  
  return splits

#normaliza, reordena o shape para o modelo a ser utilizado, separa dos dados para treino e testes
def split_sequence(sequence,split,scaler):
  x_train, y_train, x_test, y_test = list(), list(), list(), list()
  #despacotar o config_split
  n_feature,limit,w_pass,o_pass,w_fut,o_fut = split
  #parametros
  maximo = int(sequence.shape[0]*limit)

  
#Montagem das amostras para o treino
  #Somente um passo no futuro 
  #Somente uma caracteristica
  if (w_fut==1):
    if (n_feature==1):
      # print('1 - w_fut=1, feature=1')
      sequence = sequence.reshape(sequence.shape[0],1)
      # print(sequence.shape)
      sequence_norm = scaler.fit_transform(sequence)
      # sequence_norm = sequence
      
     
      for i in range(maximo-w_pass-o_fut):
        end_ix = i+w_pass
        if end_ix > len(sequence_norm)-1-o_fut:
          break
        seq_x, seq_y = sequence_norm[i:end_ix,], sequence_norm[end_ix+o_fut]
        x_train.append(seq_x)
        y_train.append(seq_y)
        
      #Montagem das amostras para o teste x normalizado y original
      for i in range(maximo-w_pass,len(sequence_norm)-o_fut):
        end_ix = i+w_pass
        if end_ix > len(sequence_norm)-1-o_fut:
          break
        seq_x, seq_y = sequence_norm[i:end_ix], sequence[end_ix+o_fut]
        x_test.append(seq_x)
        y_test.append(seq_y)

      x_train = array(x_train)
      y_train = array(y_train)
      x_test = array(x_test)
      y_test = array(y_test)
      
      # x_train = x_train.reshape(x_train.shape[0],w_pass)
      y_train = y_train.reshape(y_train.shape[0])
      # x_test = x_test.reshape(x_test.shape[0],w_pass)
      y_test = y_test.reshape(y_test.shape[0])


    #Simples mais de  uma caracteristica
    else:
      # print('3 - w_fut=1, feature>1')
      # print('fit scale')
      # print(sequence.shape)
      sequence_norm = scaler.fit_transform(sequence)
      # sequence_norm = sequence

    
      for i in range(maximo-w_pass-o_fut):
        end_ix = i+w_pass
        if end_ix > sequence_norm.shape[0]-w_pass-o_fut:
          break
        seq_x, seq_y = sequence_norm[i:end_ix,:-1], sequence_norm[end_ix+o_fut,n_feature]
        x_train.append(seq_x)
        y_train.append(seq_y)
      
      #Montagem das amostras para o teste x normalizado y original
      for i in range(maximo-w_pass,sequence_norm.shape[0]-o_fut):
        end_ix = i+w_pass
        if end_ix > sequence_norm.shape[0]-1-o_fut:
          break
        seq_x, seq_y = sequence_norm[i:end_ix,:-1], sequence[end_ix+o_fut,n_feature]
        x_test.append(seq_x)
        y_test.append(seq_y)

      #Array e reshape
      
      # print(len(x_t/rain))
      
      x_train = array(x_train)
      y_train = array(y_train)
      x_test = array(x_test)
      y_test = array(y_test)
    

      # print(y_train.shape)

      # x_train = x_train.reshape(x_train.shape[0],w_pass,n_feature)
      y_train = y_train.reshape(y_train.shape[0])
      # x_test = x_test.reshape(x_test.shape[0]*n_input)
      y_test = y_test.reshape(y_test.shape[0])


  #Mais de um passo no futuro
  else:
    #Somente uma caracteristica
    if (n_feature==1):
      # print('2 - w_fut>1, feature=1')
      sequence = sequence.reshape(sequence.shape[0],1)
      # print(sequence.shape)
      sequence_norm = scaler.fit_transform(sequence)
      # sequence_norm = sequence


      for i in range(maximo-w_pass-o_fut):
        end_ix = i+w_pass
        out_end_ix = end_ix + w_fut
        if out_end_ix > len(sequence_norm)-o_fut:
          break
        seq_x, seq_y = sequence_norm[i:end_ix], sequence_norm[end_ix+o_fut:out_end_ix+o_fut]
        x_train.append(seq_x)
        y_train.append(seq_y)
      #Montagem das amostras para o teste x normalizado y original
      for i in range(maximo-w_pass,len(sequence_norm)-o_fut):
        end_ix = i+w_pass
        out_end_ix = end_ix + w_fut
        if end_ix > len(sequence_norm)-w_fut-o_fut:
          break
        seq_x, seq_y = sequence_norm[i:end_ix], sequence[end_ix+o_fut:out_end_ix+o_fut]
        x_test.append(seq_x)
        y_test.append(seq_y)


      #Array e reshape
      x_train = array(x_train)
      y_train = array(y_train)
      x_test = array(x_test)
      y_test = array(y_test)

      # x_train = x_train.reshape(x_train.shape[0],w_pass)
      y_train = y_train.reshape(y_train.shape[0],w_fut)
      # x_test = x_test.reshape(x_test.shape[0],w_pass)
      y_test = y_test.reshape(y_test.shape[0],w_fut)


    #Mais de  uma caracteristica
    else:
      # print('4 - w_fut>1, feature>1')
      # print(sequence.shape)
      sequence_norm = scaler.fit_transform(sequence)
      # sequence_norm = sequence

      for i in range(maximo-w_pass-o_fut):
        end_ix = i+w_pass
        out_end_ix = end_ix + w_fut
        if out_end_ix > len(sequence_norm)-o_fut:
          break
        seq_x, seq_y = sequence_norm[i:end_ix,:-1], sequence_norm[end_ix+o_fut:out_end_ix+o_fut,n_feature]
        x_train.append(seq_x)
        y_train.append(seq_y)
      #Montagem das amostras para o teste x normalizado y original
      for i in range(maximo-w_pass,len(sequence_norm)-o_fut):
        end_ix = i+w_pass
        out_end_ix = end_ix + w_fut
        if end_ix > len(sequence_norm)-w_fut-o_fut:
          break
        seq_x, seq_y = sequence_norm[i:end_ix,:-1], sequence[end_ix+o_fut:out_end_ix+o_fut,n_feature]
        x_test.append(seq_x)
        y_test.append(seq_y)

      #Array e reshape
      x_train = array(x_train)
      y_train = array(y_train)
      x_test = array(x_test)
      y_test = array(y_test)
      # x_train = x_train.reshape(x_train.shape[0],n_input)
      # y_train = y_train.reshape(y_train.shape[0],w_fut)
      # x_test = x_test.reshape(x_test.shape[0],n_input)
      # y_test = y_test.reshape(y_test.shape[0],y_test.shape[1])


  # print(x_train)
  # print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)  
  # print(x_train,y_train,x_test,y_test)

  # print("split_sequence")
  return x_train,y_train,x_test,y_test,scaler

#Dados Ficticios para testar os modelos
def dataset(value,zz):
  series = array([x for x in range(100)])
  nvalue = 1
  if (value == 2):
    seq1 = series
    seq2 = array([x for x in range(1,101)])
    seq3 = array([seq1[i]+seq2[i] for i in range(len(seq1))])
    seq1 = seq1.reshape(len(seq1),1)
    seq2 = seq2.reshape(len(seq2),1)
    seq3 = seq3.reshape(len(seq3),1)
    series = hstack((seq1, seq2, seq3))
    nvalue = 3
  # print('dataset')
  return series, nvalue

def score_model(ativo,cfg_ciclo,models,cfg_norm,cfg_split,cfg_system):
  result2 = None

  if cfg_system[0]:
    result2 = grade(ativo,cfg_ciclo,models,cfg_norm,cfg_split,cfg_system)
  else:
    try:
      with catch_warnings():
        filterwarnings("ignore")
        result2 = grade(ativo,cfg_ciclo,models,cfg_norm,cfg_split,cfg_system)
    except:
        error = None

  # print('score_model')
  return result2

def model_types():
  models = ['lstm'] #'simpleLinear' 'mlp','cnn','lstm','gru','simpleRNN'
  # models = ['baseRNN'] 
  return models

def types_norms(): #
  # norms = ['standarScaler','maxAbsScaler','minMaxScaler','robustScaler'] #'normalizer','powerTransformer',
  norms = ['standarScaler']
  return norms

def getAtivos():
  # ativos = ['PETR4Daily','ABEV3Daily']
  # ativos = ['PETR4M1']
  ativos = ['PETR4Daily']
  return ativos

def main():
  #_________________________________________________________________________________
  print("GRADE PESQUISA DE MODELOS E SEUS HIPERPARAMETROS")
  #Função para separação dos dados
  print("Início")
  current_begin = datetime.datetime.now()

  #Contador universal
  cont = 0
  #hipermametros
  dt = 2

  #Parametros
  sp_mi = 90       #percentual minimo teste/validação - minimo(1)
  sp_ma = 90         #percentual máximo teste/validação - máximo(9)
  wp_mi = 10         #janela no passado  - minimo(1)
  wp_ma = 10         #janela do passado - maximo (auto ou minimo =1)
  wop_mi = 0        #offset no passado - minimo(0)
  wop_ma = 0        #offset no passado - maximo(10) depende do tamanho da janela no passado
  wf_mi = 1         #janela no futuro - minimo(1)
  wf_ma = 1         #janela no futuro - máximo(10) depende do tamanho da janela do futuro
  wof_mi = 0        #offset no futuro minimo - minimo(0)
  wof_ma = 0        #offset no futuro máximo - máximo(10)

  #Parametros dos ciclos
  ciclo_0 = 1   #repetições do modelo 
  ciclo_1 = 1   #repetições dos parametros
  ciclo_2 = 1   #repetições da grade

  debug = True
  parallel = False
  verbose = 0
  graphic = True
  salve = True
  path = '/content/gdrive/My Drive/TCC/Colab Notebooks/Testes/Predição/'

  models = model_types()


  #Configurações
  ativos = getAtivos()
  cfg_ciclo = [ciclo_0,ciclo_1,ciclo_2]
  cfg_system = [debug,graphic,salve,verbose,path,dt,cont]
  cfg_split = [sp_mi,sp_ma,wp_mi,wp_ma,wop_mi,wop_ma,wf_mi,wf_ma,wof_mi,wof_ma]
  cfg_norm = types_norms()

  #Paralelismo
  scores = list()
  if parallel:
      executor = Parallel(n_jobs=cpu_count(), backend='multiprocessing')
      tasks = (delayed(score_model)(ativo,cfg_ciclo,models,cfg_norm,cfg_split,cfg_system) for ativo in ativos)
      scores = executor(tasks)
  else:
    scores = [score_model(ativo,cfg_ciclo,models,cfg_norm,cfg_split,cfg_system) for ativo in ativos]

if __name__ == '__main__':
  main()



